[
  {
    "objectID": "index.html#computational-environments",
    "href": "index.html#computational-environments",
    "title": "Implied Motion",
    "section": "Computational Environments",
    "text": "Computational Environments\nTo be able to create the computational environments used, you must have conda and docker installed.\nFor fMRI preprocessing, I used an fMRIprep docker container, which can be accessed with the following command:\ndocker pull nipreps/fmriprep:21.0.4\nFor ROI generation, I used a FreeSurfer docker image, which you can download with:\ndocker pull freesurfer/freesurfer:7.3.0\nLastly, for the dicom to nifti conversion, parts of the ROI generation, and decoding I used a virtual environment. The full specification of the virtual environment here can be found here, and the identical environment can be created using:\nconda env create -f https://raw.githubusercontent.com/candemircan/src_implied_motion/main/environment.yml"
  },
  {
    "objectID": "index.html#note-about-paths",
    "href": "index.html#note-about-paths",
    "title": "Implied Motion",
    "section": "Note About Paths",
    "text": "Note About Paths\nIn my configuration, I had placed the folder containing the code and the data on my home directory, and the folder was called implied_motion. If you place it in a different directory, you need to modify the project_root or the PROJECT_ROOT or the PROJECTROOT variables that appear at the beginning of the scripts."
  },
  {
    "objectID": "index.html#note-about-participant-numbers",
    "href": "index.html#note-about-participant-numbers",
    "title": "Implied Motion",
    "section": "Note About Participant Numbers",
    "text": "Note About Participant Numbers\nWe use participants from 4 to 23 (inclusive). The first three participants had participated in a pilot that used a different sequence and a slightly different experimental paradigm, therefore their data is not used. Additionally participant 19 is also excluded due to half of their data missing (which was also reported by Gizem)."
  },
  {
    "objectID": "dicom_2_nifti.html",
    "href": "dicom_2_nifti.html",
    "title": "Dicom to Nifti Conversion",
    "section": "",
    "text": "First, I converted the raw data from dicom format to nifti format. For this, I used the dcm2niix tool in the following script:\n\n\n#!/bin/bash\n\nPROJECT_ROOT=$HOME/implied_motion\nDATA_RAW=$PROJECT_ROOT/data/raw\nDATA_NIFTI=$PROJECT_ROOT/data/bids\n\n\n\nfor i in 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 20 21 22 23\ndo\n\n  # get the path to the subjects new to be nifti folder\n  SUB_FOLDER=sub-$(printf %02d $i)\n  DATA_RAW_PAR=$DATA_RAW/$SUB_FOLDER\n  DATA_NIFTI_PAR=$DATA_NIFTI/$SUB_FOLDER\n\n\n  # make the folder if it does not exist\n  if [ ! -d \"$DATA_NIFTI_PAR\" ]; then\n    mkdir -p \"$DATA_NIFTI_PAR\"\n  fi\n\n  # explanation of flags\n  # -o: output directory\n  # -b: create bids sidecar (y or n)\n  # -ba: anonymise bids sidecar (y or n)\n  # last positional: input directory\n\n  dcm2niix -o \"$DATA_NIFTI_PAR\" -b y -ba y \"$DATA_RAW_PAR\"\n\ndone"
  },
  {
    "objectID": "nifti_2_bids.html#reorganise-the-nifti-files",
    "href": "nifti_2_bids.html#reorganise-the-nifti-files",
    "title": "Nifti to BIDS Conversion",
    "section": "Reorganise the Nifti Files",
    "text": "Reorganise the Nifti Files\nAt this point, the data is still messy. For a reusable and sharable format (and to be able to use it with some existing toolboxes), I rearranged the nifti data into the BIDS format. I tried to automate this process as much as possible, however the raw data was unstructured in different ways across participants in certain cases. Therefore, instead of writing 19 specialised versions of the code, I carried out some manual steps (e.g. moving structural scans from one folder to another). Running the code provided below on the raw data alone will not put the data into the BIDS format like you see it on the server.\nThe following example structure was created after taking the steps above.\nsub-01\n│\n└───func\n│   │   sub-01_task-train_run-1_sbref.json\n|   |   sub-01_task-train_run-1_sbref.nii\n│   │   sub-01_task-train_run-1_bold.json\n|   |   sub-01_task-train_run-1_bold.nii\n|   |   ...\n│   │   sub-01_task-test_run-16_sbref.json\n|   |   sub-01_task-test_run-16_sbref.nii\n│   │   sub-01_task-test_run-16_bold.json\n|   |   sub-01_task-test_run-16_bold.nii\n|\n└───anat\n|   │   sub-01_T1w.json\n|   │   sub-01_T1w.nii\n|\n└───fmap\n|   |   sub-01-magnitude1.json\n|   |   sub-01-magnitude1.nii\n|   |   sub-01-magnitude2.json\n|   |   sub-01-magnitude2.nii\n|   |   sub-01-phasediff.json\n|   |   sub-01-phasediff.nii\nExtra Note: Some participants have fieldmaps while others don’t. When available, I used them, and otherwise they are ignored\n\n\nimport os\nimport datetime\nimport json\nfrom pathlib import Path\n\n\ndef rename_nifti(\n    project_root: str,  # root directory of the project\n    subject_no: int,  # subject no\n    bold_series_size: float = 73.68783569335938,  # size of bold series images\n    bold_ref_size: float = 0.281585693359375,  # size of bold reference images for some subjects\n    second_bold_ref_size: float = 73.12533569335938,  # size of bold reference images for some subjects\n    anat_ref_size: float = 21.750335693359375,  # size of anatomical images\n    fmap_ref_size: float = 0.281585693359375,  # size of fieldmap images\n):\n\n    \"\"\"renames the files `dcm2niix` converted and puts them in bids format\"\"\"\n\n    subject_no = str(subject_no)\n\n    # define boring references\n    allowed_sizes = [\n        bold_series_size,\n        bold_ref_size,\n        anat_ref_size,\n        fmap_ref_size,\n        second_bold_ref_size,\n    ]\n    sub_folder = f\"sub-{subject_no.zfill(2)}\"\n    nifti_path = f\"{project_root}/data/bids\"\n    all_imaging_data = os.listdir(f\"{nifti_path}/{sub_folder}\")\n\n    # create the following folders if they don't exist already\n    new_folders = [\n        f\"{nifti_path}/{sub_folder}/anat\",\n        f\"{nifti_path}/{sub_folder}/func\",\n        f\"{nifti_path}/{sub_folder}/fmap\",\n    ]\n\n    for folder in new_folders:\n        try:\n            os.mkdir(folder)\n        except FileExistsError:\n            print(f\"{folder} already exists. Returning now...\")\n            return\n\n    ## rename anatomical image and the json\n\n    my_anat = [\n        file\n        for file in all_imaging_data\n        if os.path.getsize(f\"{nifti_path}/{sub_folder}/{file}\") / (1024 * 1024)\n        == anat_ref_size\n    ]\n    anat_image = [\n        file\n        for file in all_imaging_data\n        if os.path.getsize(f\"{nifti_path}/{sub_folder}/{file}\") / (1024 * 1024)\n        == anat_ref_size\n    ][0].split(\".\")[0]\n    os.rename(\n        f\"{nifti_path}/{sub_folder}/{anat_image}.nii\",\n        f\"{nifti_path}/{sub_folder}/anat/{sub_folder}_T1w.nii\",\n    )\n    os.rename(\n        f\"{nifti_path}/{sub_folder}/{anat_image}.json\",\n        f\"{nifti_path}/{sub_folder}/anat/{sub_folder}_T1w.json\",\n    )\n\n    # Organise things a bit and sorted behavioural file names based on date\n\n    beh_data = os.listdir(\n        f\"{project_root}/data/behavioural/{sub_folder}/\"\n    )  # get all files\n    beh_data = [file for file in beh_data if file.endswith(\".mat\")]  # keep mat only\n    beh_data = [x for x in beh_data if \"ROILoc\" not in x]  # remove the localiser run\n    beh_date_dict = {\n        re.search(r\"\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}\", x).group(): x for x in beh_data\n    }  # create a dict where dates are the keys & file names are the values\n    beh_data_sorted = sorted(\n        beh_date_dict.keys(),\n        key=lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d_%H-%M\"),\n    )  # get the keys in sorted format\n    beh_data_sorted = [\n        beh_date_dict[x] for x in beh_data_sorted\n    ]  # now get the file names in a sorted way\n\n    # Now sort nifti and json files you want to keep based on number\n\n    nifti_data = os.listdir(f\"{nifti_path}/{sub_folder}\")\n    nifti_bold = [x for x in nifti_data if (\"bold\") in x and (\".nii\") in x]\n    nifti_bold_json = [x for x in nifti_data if (\"bold\") in x and (\".json\") in x]\n    nifti_bold_sorted = sorted(\n        nifti_bold, key=lambda x: int(re.split(r\"(\\d+)\", x)[-2])\n    )  # split strings by numbers, take the second item from the last as the last item is the file extension\n    nifti_bold_json_sorted = sorted(\n        nifti_bold_json, key=lambda x: int(re.split(r\"(\\d+)\", x)[-2])\n    )  # same as above but for json\n\n    # Remove nifti and json files if the corresponding nifti file is not\n    # something we want (due to an interrupted run or the localisation block)\n\n    file_sizes = [\n        os.stat(f\"{nifti_path}/{sub_folder}/{x}\").st_size / (1024 * 1024)\n        for x in nifti_bold_sorted\n    ]  # get sorted file sizes (in MB)\n    illegal = [file_sizes.index(x) for x in file_sizes if x not in allowed_sizes]\n    illegal_refs = [x - 1 for x in illegal]  # reference images for the illegal images\n    illegal.extend(illegal_refs)\n    nifti_bold_sorted = [\n        nifti_bold_sorted[i] for i in range(len(nifti_bold_sorted)) if i not in illegal\n    ]\n    nifti_bold_json_sorted = [\n        nifti_bold_json_sorted[i]\n        for i in range(len(nifti_bold_json_sorted))\n        if i not in illegal\n    ]\n\n    # now match between old names and new names you create below\n\n    bold_dict = {x: [] for x in nifti_bold_sorted}\n    json_dict = {x: [] for x in nifti_bold_json_sorted}\n\n    run = 0\n    int_counter = 0  # every run is used twice in naming .nii files, one for reference the other for bold series. This is to keep track of that\n    mat_counter = 0  # need to keep seperate track of this as behavioural data file no does not match\n    run_counter = 1\n    for i, file in enumerate(nifti_bold_sorted):\n        if int_counter == 2:  # every two files\n            int_counter = 0  # new behavioural file\n            mat_counter += 1  # new behavioural file\n            run_counter += 1  # increase run counts\n\n        if not int_counter:  # check type of block for new behavioral file\n            if \"Train\" in beh_data_sorted[mat_counter]:\n                block_type = \"train\"\n            else:\n                block_type = \"test\"\n\n        write_run = run_counter\n        image_type = \"sbref\" if int_counter == 0 else \"bold\"\n        int_counter += 1\n\n        bold_dict[\n            file\n        ] = f\"sub-{subject_no.zfill(2)}_task-{block_type}_run-{write_run}_{image_type}\"\n\n    # now assign the proper names with extensions both for jsons and nii images\n    for (key_bold, value), key_json in zip(bold_dict.items(), json_dict.keys()):\n        bold_dict[key_bold] = f\"{value}.nii\"\n        json_dict[key_json] = f\"{value}.json\"\n\n    # now rename the func files\n    for cur_dict in [bold_dict, json_dict]:\n        for key, value in cur_dict.items():\n            os.rename(\n                f\"{nifti_path}/{sub_folder}/{key}\",\n                f\"{nifti_path}/{sub_folder}/func/{value}\",\n            )\n            pass\n\n    # add task name to json files\n    for file in json_dict.values():\n        side = json.load(open(f\"{nifti_path}/{sub_folder}/func/{file}\"))\n        side[\"TaskName\"] = \"test\" if \"test\" in file else \"train\"\n        with open(f\"{nifti_path}/{sub_folder}/func/{file}\", \"w\") as to_write:\n            json.dump(side, to_write)\n\n    # move fieldmap to fmap folder\n\n    e1_image = [file for file in all_imaging_data if file.endswith(\"e1.nii\")]\n\n    # not all participants have fieldmaps\n    # carry on with the following only if they have fieldmaps\n    if e1_image:\n        e1_image = e1_image[0].split(\".\")[0]\n        e2_image = [file for file in all_imaging_data if file.endswith(\"e2.nii\")][\n            0\n        ].split(\".\")[0]\n        e2_ph_image = [file for file in all_imaging_data if file.endswith(\"e2_ph.nii\")][\n            0\n        ].split(\".\")[0]\n\n        fmap_dict = {\n            e1_image: f\"{sub_folder}_magnitude1\",\n            e2_image: f\"{sub_folder}_magnitude2\",\n            e2_ph_image: f\"{sub_folder}_phasediff\",\n        }\n\n        for key, value in fmap_dict.items():\n            os.rename(\n                f\"{nifti_path}/{sub_folder}/{key}.nii\",\n                f\"{nifti_path}/{sub_folder}/fmap/{value}.nii\",\n            )\n            os.rename(\n                f\"{nifti_path}/{sub_folder}/{key}.json\",\n                f\"{nifti_path}/{sub_folder}/fmap/{value}.json\",\n            )\n            pass\n\n        # add 'intended for' to fieldmaps\n        fmap_files = os.listdir(f\"{nifti_path}/{sub_folder}/fmap/\")\n        func_files = os.listdir(f\"{nifti_path}/{sub_folder}/func/\")\n        func_files = [x for x in func_files if \"sbref\" not in x]\n        func_files = [file for file in func_files if \".nii\" in file]\n        func_files = [f\"func/{file}\" for file in func_files]\n\n        for file in fmap_files:\n            if \"json\" in file:\n                side = json.load(open(f\"{nifti_path}/{sub_folder}/fmap/{file}\"))\n                side[\"IntendedFor\"] = func_files\n                with open((f\"{nifti_path}/{sub_folder}/fmap/{file}\"), \"w\") as to_write:\n                    json.dump(side, to_write)\n\n    else:\n        os.rmdir(new_folders[2])\n\n    # remove everything else\n    for f in os.listdir(f\"{nifti_path}/{sub_folder}/\"):\n        try:\n            os.remove(os.path.join(f\"{nifti_path}/{sub_folder}/\", f))\n        except IsADirectoryError:\n            pass\n\n    return None\n\n\nif __name__ == \"__main__\":\n    par_list = list(range(4, 24))\n    par_list.remove(19)  # the data from participant 19 is missing\n    home_dir = Path.home()\n    project_dir = f\"{home_dir}/implied_motion\"\n\n    for participant in par_list:\n\n        rename_nifti(project_root=project_dir, subject_no=participant)"
  },
  {
    "objectID": "nifti_2_bids.html#create-events-files",
    "href": "nifti_2_bids.html#create-events-files",
    "title": "Nifti to BIDS Conversion",
    "section": "Create Events Files",
    "text": "Create Events Files\nTo create the event files used in my GLM, I used the following code below, which create one .tsv file per functional run and saves it under the relevant subject’s func folder in the data/bids. For the decoding, we tried two different kinds of design matrices. The examples for the two kind are:\nIf expanded==False, a table of the following type is created:\n\n\n\nonset\nduration\nmotion direction\n\n\n\n\n10\n15\nleft\n\n\n30\n15\nforward\n\n\n…\n…\n…\n\n\n\nIf expanded==True, a table of this type is created:\n\n\n\nonset\nduration\nmotion direction\n\n\n\n\n10\n15\nleft_1\n\n\n30\n15\nforward_1\n\n\n50\n15\nleft_2\n\n\n…\n…\n…\n\n\n\n\n\nimport os\nfrom pathlib import Path\nfrom collections import namedtuple\n\nfrom scipy.io import loadmat\nimport pandas as pd\n\n# I prepare two kinds of events files, expanded and compact, which are explained in `get_regressors`.\n# These are saved under the `bids/<<SUBJECT>>/func` folder\n\n\ndef get_regressors(\n    project_root: str,  # root directory of the project\n    subj: int,  # subject no\n    expanded: bool,  # boolean value to determine whether to create expanded or compact regressor files\n    direction_coding: dict = {\n        1: \"forward\",\n        2: \"backward\",\n        3: \"right\",\n        4: \"left\",\n    },  # how different directions are coded in the .mat files\n    n_dummies: int = 8,  # number of dummy images\n    tr: float = 1.2,  # repetition time\n):\n\n    \"\"\"\n    writes events files in bids compliant tsv format\n\n    \"\"\"\n    sub_str = f\"sub-{str(subj).zfill(2)}\"\n    file_dir = f\"{project_root}/data/behavioural/{sub_str}\"\n\n    # get files\n    all_files = os.listdir(file_dir)\n    # keep the ones we are interested in (i.e. those that contain \"Train\" or \"Test\")\n    legal_files = [file for file in all_files if (\"Train\" in file) or (\"Test\" in file)]\n    # sort them as regressors will be appended in a chronological order\n    legal_files.sort()\n\n    # collect Row in rows to create a dataframe\n    Row = namedtuple(\"Row\", [\"onset\", \"duration\", \"trial_type\"])\n\n    # also get the associated run file names as the names will be used\n    # for tsv\n\n    run_files = os.listdir(f\"{project_root}/data/bids/{sub_str}/func/\")\n    run_files = sorted(run_files, key=lambda x: int(re.split(r\"(\\d+)\", x)[-2]))\n    run_files = [x.split(\"bold.nii\")[0] for x in run_files if \"bold.nii\" in x]\n\n    for file, img_file in zip(legal_files, run_files):\n        complete_path = os.path.join(file_dir, file)\n        mat = loadmat(complete_path)[\"logs\"][0, 0]  # mat files have a funny format\n        motion_type = \"train\" if \"Train\" in file else \"test\"\n        rows = []\n\n        # subtract is the starting point of scans, which is a nonzero value\n        # it is used to set the first regressor at 0\n        # note that it starts from the second block as the first block is to be discarded\n\n        subtract = mat[\"blockOnsets\"][0, 0] - n_dummies * tr\n\n        counter = {\"forward\": 1, \"backward\": 1, \"left\": 1, \"right\": 1}\n\n        for block in range(mat[\"blockOnsets\"].shape[1]):\n            if block == 0:  # don't record anything for the first block\n                pass\n            else:\n\n                # get the direction from matching value to dictionary\n                trial_type = direction_coding[mat[\"blockSeq\"][0, block]]\n                if expanded:\n                    trial_type_write = (\n                        f\"{trial_type}_{counter[trial_type]}\"\n                        if motion_type == \"train\"\n                        else f\"{trial_type}_{counter[trial_type]}\"\n                    )\n                else:\n                    trial_type_write = (\n                        f\"{trial_type}\" if motion_type == \"train\" else f\"{trial_type}\"\n                    )\n                counter[trial_type] += 1\n\n                # center onsets at 0 and add previous runs' times\n                onset = mat[\"blockOnsets\"][0, block] - subtract\n\n                # the following is to accomodate for the slightly differently\n                # coded structure fields in the .mat file\n                if motion_type == \"train\":\n                    duration = (\n                        mat[\"blockOffsets\"][0, block] - mat[\"blockOnsets\"][0, block]\n                    )\n                else:\n                    duration = mat[\"bintOnset\"][0, block] - mat[\"blockOnsets\"][0, block]\n\n                # for the df\n                rows.append(\n                    Row(\n                        onset=onset,\n                        duration=duration,\n                        trial_type=trial_type_write,\n                    )\n                )\n\n        df = pd.DataFrame(rows)\n\n        if expanded:\n            write_path = (\n                f\"{project_root}/data/bids/{sub_str}/func/expanded_{img_file}events.tsv\"\n            )\n        else:\n            write_path = f\"{project_root}/data/bids/{sub_str}/func/{img_file}events.tsv\"\n\n        df.to_csv(write_path, index=False, sep=\"\\t\")\n\n    return None\n\n\nif __name__ == \"__main__\":\n\n    par_list = list(range(4, 24))\n    par_list.remove(19)  # the data from participant 19 is missing\n    home_dir = Path.home()\n    project_dir = f\"{home_dir}/implied_motion\"\n\n    for par in par_list:\n        get_regressors(project_root=project_dir, subj=par, expanded=True)\n        get_regressors(project_root=project_dir, subj=par, expanded=False)"
  },
  {
    "objectID": "preprocessing.html",
    "href": "preprocessing.html",
    "title": "Preprocessing MRI Data",
    "section": "",
    "text": "The following two scripts were used to preprocess the imaging data. The first script calls the second one for each participant and submits seperate jobs for parallelisation on the server.\n\n\n#!/bin/bash\n\n# submitting fmriprep for all participants to the server\n# processing will be done in parallel\n\nPROJECT_ROOT=$HOME/implied_motion\n\n\nfor par in 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 20 21 22 23\ndo\n    SUB_FOLDER=sub-$(printf %02d $par)\n    qsub -F \"--participant '$SUB_FOLDER'\" \\\n    \"$PROJECT_ROOT\"/src/sh/fMRIPrep.pbs \ndone\n\n\n\n\n#PBS -S /bin/bash\n#PBS -q shared\n#PBS -l nodes=1:ppn=8\n#PBS -l mem=8gb\n#PBS -N fmriprep_can\n#PBS -o fmriprep_can.out\n#PBS -e fmriprep_can.err\n#PBS -l walltime=02:00:00:00\n#PBS -m abe\n#PBS -M can.demircan@tutanota.com\n\n\n\nwhile [[ \"$#\" -gt 0 ]]\ndo case $1 in\n    -p|--participant) participant=\"$2\"\nesac\nshift\ndone\n\n\nproject_root=$HOME/implied_motion\nlicense=$project_root/data/derivatives/fMRIprep/sourcedata/freesurfer/license.txt\ninput=$project_root/data/bids\noutput=$project_root/data/derivatives/fMRIprep/\n\n\ndocker run --user \"$(id -u):$(id -g)\" \\\n-v \"$license\":/opt/freesurfer/license.txt \\\n-v \"$input\":/data:ro \\\n-v \"$output\":/out nipreps/fmriprep:latest /data \\\n/out participant --output-spaces anat func --nthreads 8 --dummy-scans 8 --participant_label \"$participant\""
  },
  {
    "objectID": "roi.html",
    "href": "roi.html",
    "title": "ROI Extraction",
    "section": "",
    "text": "We extracted the following ROIs from the Wang et al. 2015 Atlas:\n\n\n\nEarly Visual Areas\n\nV1\nV2\nV3\n\nMotion Areas\n\nV3a\nhMT\nMST\n\n\n\n\nObject Areas\n\nV3b\nhV4\nLO1\nLO2\nVO1\nVO2\n\nParietal Areas\n\nIPS (posterior)\nIPS (anterior)\nSPL1\n\n\n\n\nFirst, the following was code to extract all the relevant ROIs for each participant.\n\n\n#!/bin/bash\n\n\nPROJECT_ROOT=$HOME/implied_motion\n\nDOCKER_FREESURFER_ROOT=/home/data/derivatives/fMRIprep/sourcedata/freesurfer\n\ndeclare -A ROIS=([\"01\"]=\"V1v\" \\\n    [\"02\"]=\"V1d\" \\\n    [\"03\"]=\"V2v\" \\\n    [\"04\"]=\"V2d\" \\\n    [\"05\"]=\"V3v\" \\\n    [\"06\"]=\"V3d\" \\\n    [\"07\"]=\"hV4\" \\\n    [\"08\"]=\"VO1\" \\\n    [\"09\"]=\"VO2\" \\\n    [\"10\"]=\"PHC1\" \\\n    [\"11\"]=\"PHC2\" \\\n    [\"12\"]=\"MST\" \\\n    [\"13\"]=\"hMT\" \\\n    [\"14\"]=\"LO2\" \\\n    [\"15\"]=\"LO1\" \\\n    [\"16\"]=\"V3b\" \\\n    [\"17\"]=\"V3a\" \\\n    [\"18\"]=\"IPS0\" \\\n    [\"19\"]=\"IPS1\" \\\n    [\"20\"]=\"IPS2\" \\\n    [\"21\"]=\"IPS3\" \\\n    [\"22\"]=\"IPS4\" \\\n    [\"23\"]=\"IPS5\" \\\n    [\"24\"]=\"SPL1\" \\\n    [\"25\"]=\"FEF\")\n\n# define freesurfer environment variables\nFS_LICENSE=$DOCKER_FREESURFER_ROOT/license.txt \nFREESURFER_HOME=/usr/local/freesurfer/bin/freesurfer\n\n\nfor participant in 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 20 21 22 23\ndo\n\n    # get subject folder name\n    SUB_FOLDER=sub-$(printf %02d \"$participant\")\n\n    # name the fs container so u can exec commands on it\n    FS_CONTAINER_NAME=agbartels_freesurfer_can_$SUB_FOLDER\n        \n    # define data related variables\n    THIS_SUBJECT_DIR=$DOCKER_FREESURFER_ROOT/$SUB_FOLDER\n    THIS_SUBJECT_MEAN=(\"$PROJECT_ROOT\"/data/derivatives/fMRIprep/\"$SUB_FOLDER\"/func/\"$SUB_FOLDER\"_task*1_space-T1w_boldref.nii.gz)\n    THIS_SUBJECT_MEAN_FILE=\"${THIS_SUBJECT_MEAN##*/}\"\n    THIS_SUBJECT_MEAN_DOCKER=/home/data/derivatives/fMRIprep/$SUB_FOLDER/func/$THIS_SUBJECT_MEAN_FILE\n\n    # make ROI directory \n    PAR_ROI_FOLDER=\"$PROJECT_ROOT\"/data/derivatives/wang_2015/\"$SUB_FOLDER\"\n\n    if [ ! -d \"$PAR_ROI_FOLDER\" ]; then\n        mkdir -p \"$PAR_ROI_FOLDER\"\n    fi\n\n    # generate surface labels from Wang et al. 2015\n    if [[ \"$participant\" == 7 ]]\n    then\n        python -m neuropythy atlas --atlases wang15 --verbose \"$SUB_FOLDER\"\n    fi\n\n    docker run \\\n    --detach \\\n    --name \"$FS_CONTAINER_NAME\" \\\n    --interactive \\\n    --tty \\\n    --rm \\\n    --user \"$(id -u):$(id -g)\" \\\n    --volume \"${PROJECT_ROOT}:/home\" \\\n    --env FS_LICENSE=$FS_LICENSE \\\n    --env SUBJECTS_DIR=$DOCKER_FREESURFER_ROOT \\\n    --env FREESURFER_HOME=$FREESURFER_HOME \\\n    --cpus=2 \\\n    freesurfer/freesurfer:7.3.0\n\n    # register mean epi to surface\n\n    docker exec \"$FS_CONTAINER_NAME\" \\\n    tkregister2_cmdl --mov \"$THIS_SUBJECT_MEAN_DOCKER\" \\\n    --s \"$SUB_FOLDER\" --regheader --reg $DOCKER_FREESURFER_ROOT/\"$SUB_FOLDER\"/register_\"$SUB_FOLDER\".dat\n\n\n    for hemisphere in lh rh\n    do\n        for roi_no in \"${!ROIS[@]}\"\n        do\n\n            echo running participant \"$participant\" roi \"${ROIS[$roi_no]}\" \"$hemisphere\"\n        # get labels for the ROIs\n            docker exec \"$FS_CONTAINER_NAME\" \\\n            mri_vol2label --i \"$THIS_SUBJECT_DIR\"/surf/$hemisphere.wang15_mplbl.mgz \\\n            --id \"$roi_no\" --l \"$THIS_SUBJECT_DIR\"/label/$hemisphere.wang2015atlas.\"${ROIS[$roi_no]}\".label \\\n            --surf-path \"$THIS_SUBJECT_DIR\"/surf/$hemisphere.inflated\n\n        #  create roi masks\n            docker exec \"$FS_CONTAINER_NAME\" \\\n            mri_label2vol --label \"$THIS_SUBJECT_DIR\"/label/$hemisphere.wang2015atlas.\"${ROIS[$roi_no]}\".label  \\\n            --temp \"$THIS_SUBJECT_MEAN_DOCKER\" \\\n            --reg $DOCKER_FREESURFER_ROOT/\"$SUB_FOLDER\"/register_\"$SUB_FOLDER\".dat \\\n            --fillthresh 0 --proj frac 0 1 .1 \\\n            --subject \"$SUB_FOLDER\" --hemi $hemisphere \\\n            --o /home/data/derivatives/wang_2015/\"$SUB_FOLDER\"/$hemisphere-\"${ROIS[$roi_no]}\"-roi.nii\n        done\n    done\n    docker stop \"$FS_CONTAINER_NAME\"\ndone\n\n\nThen, the following code was used to a) remove overlapping voxels from all ROIs b) merge some of the ‘raw’ ROIs given by the atlas.\n\n\nfrom pathlib import Path\nimport os\n\nimport nibabel as nib\nimport numpy as np\nfrom nilearn import image\n\n\ndef merge_rois(project_root: str, participant: int):\n\n    \"\"\"\n    Merge the left and right masks and regions that we want to look at together.\n    Also remove all overlapping voxels across all masks.\n    \"\"\"\n\n    print(f\"creating roi masks for participant {participant}\")\n\n    participant = f\"sub-{str(participant).zfill(2)}\"\n    wang_roi = f\"{project_root}/data/derivatives/wang_2015/{participant}\"\n    participant_roi = f\"{project_root}/data/derivatives/fMRIprep/{participant}/ROI\"\n\n    if not os.path.exists(participant_roi):\n        os.mkdir(participant_roi)\n\n    ROIs = {\n        \"V3a\": [],\n        \"V3b\": [],\n        \"hV4\": [],\n        \"LO1\": [],\n        \"LO2\": [],\n        \"VO1\": [],\n        \"VO2\": [],\n        \"hMT\": [],\n        \"MST\": [],\n        \"SPL1\": []\n    }\n\n    groups = {\n        \"V1\": [\"V1v\", \"V1d\"],\n        \"V2\": [\"V2v\", \"V2d\"],\n        \"V3\": [\"V3v\", \"V3d\"],\n        \"IPS_posterior\": [\"IPS0\", \"IPS1\", \"IPS2\"],\n        \"IPS_anterior\": [\"IPS3\", \"IPS4\", \"IPS5\"],\n    }\n\n    # merge left and right for the single rois\n    for key in ROIs.keys():\n        left_image = image.load_img(\n            f\"{wang_roi}/lh-{key}-roi.nii\"\n        )\n        right_image = image.load_img(\n            f\"{wang_roi}/rh-{key}-roi.nii\"\n        )\n        ROIs[key] = image.math_img(\"img1 + img2\", img1=left_image, img2=right_image)\n\n    # get the rois to be combined and merge left and right for all those images\n    for new_img, old_imgs in groups.items():\n        if new_img in [\"V1\", \"V2\", \"V3\"]:\n            ROIs[new_img] = image.math_img(\n                \"img1 + img2 + img3 + img4\",\n                img1=f\"{wang_roi}/lh-{old_imgs[0]}-roi.nii\",\n                img2=f\"{wang_roi}/lh-{old_imgs[1]}-roi.nii\",\n                img3=f\"{wang_roi}/rh-{old_imgs[0]}-roi.nii\",\n                img4=f\"{wang_roi}/rh-{old_imgs[1]}-roi.nii\",\n            )\n        else:\n            ROIs[new_img] = image.math_img(\n                \"img1 + img2 + img3 + img4 + img5 + img6\",\n                img1=f\"{wang_roi}/lh-{old_imgs[0]}-roi.nii\",\n                img2=f\"{wang_roi}/lh-{old_imgs[1]}-roi.nii\",\n                img3=f\"{wang_roi}/lh-{old_imgs[2]}-roi.nii\",\n                img4=f\"{wang_roi}/rh-{old_imgs[0]}-roi.nii\",\n                img5=f\"{wang_roi}/rh-{old_imgs[1]}-roi.nii\",\n                img6=f\"{wang_roi}/rh-{old_imgs[2]}-roi.nii\",\n            )\n\n    # now remove overlapping voxels in all images\n    ROI_list = list(ROIs.values())\n    no_rois = len(ROI_list)\n    for i, key in enumerate(ROIs.keys()):\n\n        # take img of interest\n        main_img = ROI_list[i]\n\n        compare_img = np.logical_xor.reduce([x.get_fdata() for x in ROI_list])\n        new_roi_array = np.logical_and(main_img.get_fdata(), compare_img).astype(float)\n        ROIs[key] = nib.Nifti1Image(new_roi_array, main_img.affine)\n\n    # save the masks\n    for key, value in ROIs.items():\n        nib.save(value, f\"{participant_roi}/{key}.nii\")\n    \n    return None\n\nif __name__ == \"__main__\":\n    par_list = list(range(4, 24))\n    par_list.remove(19)  # the data from participant 19 is missing\n    home_dir = Path.home()\n    project_dir = f\"{home_dir}/implied_motion\"\n\n    for par in par_list:\n        merge_rois(project_root=project_dir, participant=par)"
  },
  {
    "objectID": "decode_within.html#decoding-real-motion",
    "href": "decode_within.html#decoding-real-motion",
    "title": "Decoding Within Category",
    "section": "Decoding Real Motion",
    "text": "Decoding Real Motion\n\nUsing the Expanded Design Matrix\n\n\n\nUsing the Compact Design Matrix"
  },
  {
    "objectID": "decode_within.html#decoding-implied-motion",
    "href": "decode_within.html#decoding-implied-motion",
    "title": "Decoding Within Category",
    "section": "Decoding Implied Motion",
    "text": "Decoding Implied Motion\n\nUsing the Expanded Design Matrix\n\n\n\nUsing the Compact Design Matrix"
  },
  {
    "objectID": "decode_within.html#code-used",
    "href": "decode_within.html#code-used",
    "title": "Decoding Within Category",
    "section": "Code Used",
    "text": "Code Used\n\n\n#!/gpfs01/bartels/user/cdemircan/miniconda3/envs/lr_bartels/bin/python\n\n\nimport argparse\nimport glob\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import LeaveOneGroupOut, permutation_test_score\n\nfrom nilearn.glm.first_level import FirstLevelModel\nfrom nilearn.maskers import NiftiMasker\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--workingdir\", \"-w\")\nparser.add_argument(\"--sub\", \"-s\")\nparser.add_argument(\"--roi\", \"-r\")\nparser.add_argument(\"--targetdecode\", \"-t\")\nparser.add_argument(\"--design\", \"-d\")\nparser.add_argument(\"--permutations\", \"-p\")\nparser.add_argument(\"--condition\", \"-c\")\n\nargs = parser.parse_args()\nproject_root = args.workingdir\nparticipant = f\"sub-{str(args.sub).zfill(2)}\"\nroi = args.roi\ntarget_decode = args.targetdecode\ndesign = args.design\npermutations = int(args.permutations)\n\ncondition_dict = {\"real\": \"train\", \"implied\": \"test\"}\ncondition = condition_dict[args.condition]\n\nfunctional_path = f\"{project_root}/data/derivatives/fMRIprep/{participant}/func\"\n\ntarget_event_dict = {\"lr\": [\"left\", \"right\"], \"fb\": [\"forward\", \"backward\"]}\ntarget_events = target_event_dict[target_decode]\n\nTR = 1.2\n\n# get file names\nfunc_path = f\"{project_root}/data/derivatives/fMRIprep/{participant}/func\"\n\nif design == \"compact\":\n    event_files = glob.glob(\n        f\"{project_root}/data/bids/{participant}/func/{participant}*events.tsv\"\n    )\nelif design == \"expanded\":\n    event_files = glob.glob(\n        f\"{project_root}/data/bids/{participant}/func/expanded*events.tsv\"\n    )\n\ntarget_files = glob.glob(\n    f\"{project_root}/data/bids/{participant}/func/{participant}*events.tsv\"\n)\nconf_files = glob.glob(f\"{func_path}/*confounds_timeseries.tsv\")\nfunc_files = glob.glob(f\"{func_path}/*T1w_desc-preproc_bold.nii.gz\")\n\n# filter by motion type of interest\ntarget_files = [x for x in target_files if condition in x]\nevent_files = [x for x in event_files if condition in x]\nconf_files = [x for x in conf_files if condition in x]\nfunc_files = [x for x in func_files if condition in x]\n\n# sort them in the right order\ntarget_files = sorted(\n    target_files, key=lambda x: int(x.split(\"run-\")[1].split(\"_events\")[0])\n)\nevent_files = sorted(\n    event_files, key=lambda x: int(x.split(\"run-\")[1].split(\"_events\")[0])\n)\nconf_files = sorted(\n    conf_files, key=lambda x: int(x.split(\"run-\")[1].split(\"_desc\")[0])\n)\nfunc_files = sorted(\n    func_files, key=lambda x: int(x.split(\"run-\")[1].split(\"_space\")[0])\n)\n\n# read the files\ntargets = [pd.read_table(x) for x in target_files]\ntargets = [\n    x[(x[\"trial_type\"] == target_events[0]) | (x[\"trial_type\"] == target_events[1])]\n    for x in targets\n]\n\nevents = [pd.read_table(x) for x in event_files]\nconfs = [\n    pd.read_table(x)[[\"trans_x\", \"trans_y\", \"trans_z\", \"rot_x\", \"rot_y\", \"rot_z\"]]\n    for x in conf_files\n]\n\n# get the ROI mask\n\nroi_mask = f\"{project_root}/data/derivatives/fMRIprep/{participant}/ROI/{roi}.nii\"\n\neffects = []\nconditions_label = []\nsession_label = []\n\nfor session in range(len(func_files)):\n\n    glm = FirstLevelModel(\n        t_r=TR,\n        mask_img=roi_mask,\n        high_pass=0.01,\n        hrf_model=\"spm\",\n        smoothing_fwhm=None,\n        n_jobs=-1,\n        memory=None,\n    )\n\n    # fit the glm\n    glm.fit(func_files[session], events=events[session], confounds=confs[session])\n\n    # set up contrasts: one per condition\n    conditions = events[session][\n        events[session][\"trial_type\"].str.startswith(target_events[0])\n        | (events[session][\"trial_type\"].str.startswith(target_events[1]))\n    ][\"trial_type\"].unique()\n\n    # for the expanded glm targets need to repeat\n    if design == \"compact\":\n        current_targets = conditions\n    elif design == \"expanded\":\n        current_targets = []\n        for cond in conditions:\n            current_targets.append(cond.split(\"_\")[0])\n\n    # get z scored betas and labels with associated functional runs\n    for target, condition in zip(current_targets, conditions):\n\n        effects.append(glm.compute_contrast(condition, output_type=\"z_score\"))\n        conditions_label.append(target)\n        session_label.append(func_files[session])\n\nn_runs = 8\nn_conditions = 8 if design == \"expanded\" else 2\nnifti_masker = NiftiMasker(\n    mask_img=roi_mask,\n    standardize=False,\n    runs=session_label,\n    smoothing_fwhm=None,\n    memory_level=1,\n)\n\nX = nifti_masker.fit_transform(effects)\ny = np.array(conditions_label)\ngroups = [[x] * n_conditions for x in range(n_runs)]\ngroups = [item for items in groups for item in items]\n\nclassifier = LogisticRegression(penalty=\"l2\", C=0.5, max_iter=4000, solver=\"lbfgs\")\n\ndecode_pipeline = Pipeline([(\"logistic_regression\", classifier)])\nlogo = LeaveOneGroupOut()\n\nreal_score, permutation_scores, _ = permutation_test_score(\n    decode_pipeline,\n    X,\n    y,\n    n_permutations=permutations,\n    groups=groups,\n    cv=logo,\n    random_state=1234,\n    scoring=\"accuracy\",\n    n_jobs=-1,\n)\n\npermutation_scores_list = permutation_scores.tolist()\npermutation_scores_list.append(real_score)\n\ndecode_dict = {\n    \"accuracy\": permutation_scores_list,\n    \"type\": [\"permutation\"] * permutations + [\"real\"],\n    \"participant\": [participant] * (permutations + 1),\n    \"roi\": [roi] * (permutations + 1),\n    \"design\": [design] * (permutations + 1),\n    \"target\": [target_decode] * (permutations + 1),\n}\n\ndf = pd.DataFrame(decode_dict)\n\ndf.to_csv(\n    f\"{project_root}/data/decoding/{args.condition}/{participant}_{target_decode}_{roi}_{design}.csv\",\n    index=False,\n)"
  },
  {
    "objectID": "decode_cross.html#code-used",
    "href": "decode_cross.html#code-used",
    "title": "Cross Decoding",
    "section": "Code Used",
    "text": "Code Used\n\n\n#!/gpfs01/bartels/user/cdemircan/miniconda3/envs/lr_bartels/bin/python\n\nimport argparse\nimport glob\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import PredefinedSplit, permutation_test_score\n\nfrom nilearn import image\nfrom nilearn.glm.first_level import FirstLevelModel\nfrom nilearn.maskers import NiftiMasker\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--workingdir\", \"-w\")\nparser.add_argument(\"--sub\", \"-s\")\nparser.add_argument(\"--roi\", \"-r\")\nparser.add_argument(\"--targetdecode\", \"-t\")\nparser.add_argument(\"--design\", \"-d\")\nparser.add_argument(\"--permutations\", \"-p\")\n\nargs = parser.parse_args()\nproject_root = args.workingdir\nparticipant = f\"sub-{str(args.sub).zfill(2)}\"\nroi = args.roi\ntarget_decode = args.targetdecode\ndesign = args.design\npermutations = int(args.permutations)\n\nfunctional_path = f\"{project_root}/data/derivatives/fMRIprep/{participant}/func\"\n\n\ntarget_event_dict = {\"lr\": [\"left\", \"right\"], \"fb\": [\"forward\", \"backward\"]}\ntarget_events = target_event_dict[target_decode]\n\nTR = 1.2\n\n# get file names\nfunc_path = f\"{project_root}/data/derivatives/fMRIprep/{participant}/func\"\n\nif design == \"compact\":\n    event_files = glob.glob(\n        f\"{project_root}/data/bids/{participant}/func/{participant}*events.tsv\"\n    )\nelif design == \"expanded\":\n    event_files = glob.glob(\n        f\"{project_root}/data/bids/{participant}/func/expanded*events.tsv\"\n    )\n\ntarget_files = glob.glob(\n    f\"{project_root}/data/bids/{participant}/func/{participant}*events.tsv\"\n)\nconf_files = glob.glob(f\"{func_path}/*confounds_timeseries.tsv\")\nfunc_files = glob.glob(f\"{func_path}/*T1w_desc-preproc_bold.nii.gz\")\n\n# split into train (real) and test (implied)\n\ntarget_files_train = [x for x in target_files if \"train\" in x]\ntarget_files_test = [x for x in target_files if \"test\" in x]\n\nevent_files_train = [x for x in event_files if \"train\" in x]\nevent_files_test = [x for x in event_files if \"test\" in x]\n\nconf_files_train = [x for x in conf_files if \"train\" in x]\nconf_files_test = [x for x in conf_files if \"test\" in x]\n\nfunc_files_train = [x for x in func_files if \"train\" in x]\nfunc_files_test = [x for x in func_files if \"test\" in x]\n\n\n# sort them in the right order\ntarget_files_train = sorted(\n    target_files_train, key=lambda x: int(x.split(\"run-\")[1].split(\"_events\")[0])\n)\nevent_files_train = sorted(\n    event_files_train, key=lambda x: int(x.split(\"run-\")[1].split(\"_events\")[0])\n)\nconf_files_train = sorted(\n    conf_files_train, key=lambda x: int(x.split(\"run-\")[1].split(\"_desc\")[0])\n)\nfunc_files_train = sorted(\n    func_files_train, key=lambda x: int(x.split(\"run-\")[1].split(\"_space\")[0])\n)\n\ntarget_files_test = sorted(\n    target_files_test, key=lambda x: int(x.split(\"run-\")[1].split(\"_events\")[0])\n)\nevent_files_test = sorted(\n    event_files_test, key=lambda x: int(x.split(\"run-\")[1].split(\"_events\")[0])\n)\nconf_files_test = sorted(\n    conf_files_test, key=lambda x: int(x.split(\"run-\")[1].split(\"_desc\")[0])\n)\nfunc_files_test = sorted(\n    func_files_test, key=lambda x: int(x.split(\"run-\")[1].split(\"_space\")[0])\n)\n\n# read the files\ntargets_train = [pd.read_table(x) for x in target_files_train]\ntargets_train = [\n    x[(x[\"trial_type\"] == target_events[0]) | (x[\"trial_type\"] == target_events[1])]\n    for x in targets_train\n]\n\ntargets_test = [pd.read_table(x) for x in target_files_test]\ntargets_test = [\n    x[(x[\"trial_type\"] == target_events[0]) | (x[\"trial_type\"] == target_events[1])]\n    for x in targets_test\n]\n\nevents_train = [pd.read_table(x) for x in event_files_train]\nevents_test = [pd.read_table(x) for x in event_files_test]\n\nconfs_train = [\n    pd.read_table(x)[[\"trans_x\", \"trans_y\", \"trans_z\", \"rot_x\", \"rot_y\", \"rot_z\"]]\n    for x in conf_files_train\n]\nconfs_test = [\n    pd.read_table(x)[[\"trans_x\", \"trans_y\", \"trans_z\", \"rot_x\", \"rot_y\", \"rot_z\"]]\n    for x in conf_files_test\n]\n\n# get the ROI mask\n\nroi_mask = f\"{project_root}/data/derivatives/fMRIprep/{participant}/ROI/{roi}.nii\"\neffects_train = []\nconditions_label_train = []\nsession_label_train = []\n\nfor session in range(len(func_files_train)):\n\n    glm = FirstLevelModel(\n        t_r=TR,\n        mask_img=roi_mask,\n        high_pass=0.01,\n        hrf_model=\"spm\",\n        smoothing_fwhm=None,\n        n_jobs=-1,\n        memory=None,\n    )\n\n    # fit the glm\n    glm.fit(\n        func_files_train[session],\n        events=events_train[session],\n        confounds=confs_train[session],\n    )\n\n    # set up contrasts: one per condition\n    conditions = events_train[session][\n        events_train[session][\"trial_type\"].str.startswith(target_events[0])\n        | (events_train[session][\"trial_type\"].str.startswith(target_events[1]))\n    ][\"trial_type\"].unique()\n\n    # for the expanded glm targets need to repeat\n    if design == \"compact\":\n        current_targets = conditions\n    elif design == \"expanded\":\n        current_targets = []\n        for cond in conditions:\n            current_targets.append(cond.split(\"_\")[0])\n\n    # get z scored betas and labels with associated functional runs\n    for target, condition in zip(current_targets, conditions):\n\n        effects_train.append(glm.compute_contrast(condition, output_type=\"z_score\"))\n        conditions_label_train.append(target)\n        session_label_train.append(func_files_train[session])\n\neffects_test = []\nconditions_label_test = []\nsession_label_test = []\n\nfor session in range(len(func_files_test)):\n\n    glm = FirstLevelModel(\n        t_r=TR,\n        mask_img=roi_mask,\n        high_pass=0.01,\n        hrf_model=\"spm\",\n        smoothing_fwhm=None,\n        n_jobs=-1,\n        memory=None,\n    )\n\n    # fit the glm\n    glm.fit(\n        func_files_test[session],\n        events=events_test[session],\n        confounds=confs_test[session],\n    )\n\n    # set up contrasts: one per condition\n    conditions = events_test[session][\n        events_test[session][\"trial_type\"].str.startswith(target_events[0])\n        | (events_test[session][\"trial_type\"].str.startswith(target_events[1]))\n    ][\"trial_type\"].unique()\n\n    # for the expanded glm targets need to repeat\n    if design == \"compact\":\n        current_targets = conditions\n    elif design == \"expanded\":\n        current_targets = []\n        for cond in conditions:\n            current_targets.append(cond.split(\"_\")[0])\n\n    # get z scored betas and labels with associated functional runs\n    for target, condition in zip(current_targets, conditions):\n\n        effects_test.append(glm.compute_contrast(condition, output_type=\"z_score\"))\n        conditions_label_test.append(target)\n        session_label_test.append(func_files_test[session])\n\nn_runs = 8\nn_conditions = 8 if design == \"expanded\" else 2\n\nnifti_masker_train = NiftiMasker(\n    mask_img=roi_mask,\n    standardize=False,\n    runs=session_label_train,\n    smoothing_fwhm=None,\n    memory_level=1,\n)\n\nX_train = nifti_masker_train.fit_transform(effects_train)\ncv_train_split_idx = np.zeros(X_train.shape[0])\ny_train = np.array(conditions_label_train)\n\nnifti_masker_test = NiftiMasker(\n    mask_img=roi_mask,\n    standardize=False,\n    runs=session_label_test,\n    smoothing_fwhm=None,\n    memory_level=1,\n)\n\nX_test = nifti_masker_test.fit_transform(effects_test)\ncv_test_split_idx = np.ones(X_test.shape[0])\ny_test = np.array(conditions_label_test)\n\nX = np.concatenate([X_train,X_test])\ny = np.concatenate([y_train,y_test])\ncv_split_idx = np.concatenate([cv_train_split_idx,cv_test_split_idx])\npredefined_split = PredefinedSplit(cv_split_idx)\n\n\ngroups = [[x] * n_conditions for x in range(n_runs)]\ngroups = [item for items in groups for item in items]\ngroups += groups\n\nclassifier = LogisticRegression(penalty=\"l2\", C=0.5, max_iter=4000, solver=\"lbfgs\")\n\ndecode_pipeline = Pipeline([(\"logistic_regression\", classifier)])\n\n\nreal_score, permutation_scores, _ = permutation_test_score(\n    decode_pipeline,\n    X,\n    y,\n    n_permutations=permutations,\n    cv=predefined_split,\n    groups=groups,\n    random_state=1234,\n    n_jobs=-1\n)\n\npermutation_scores_list = permutation_scores.tolist()\npermutation_scores_list.append(real_score)\n\ndecode_dict = {\n    \"accuracy\": permutation_scores_list,\n    \"type\": [\"permutation\"] * permutations + [\"real\"],\n    \"participant\": [participant] * (permutations + 1),\n    \"roi\": [roi] * (permutations + 1),\n    \"design\": [design] * (permutations + 1),\n    \"target\": [target_decode] * (permutations + 1),\n}\n\ndf = pd.DataFrame(decode_dict)\n\ndf.to_csv(\n    f\"{project_root}/data/decoding/cross/{participant}_{target_decode}_{roi}_{design}.csv\",\n    index=False,\n)"
  },
  {
    "objectID": "unused.html",
    "href": "unused.html",
    "title": "Unused",
    "section": "",
    "text": "I also have some unused analysis code that I stopped running due to time constraints. The following code performs the same permutation based decoding analysis described earlier, however it also fits the hyperparameters using nested cross-validation. The reason it takes so long to run is because it doesn’t only fit hyperparameters for the each fold of the actual data, but also for those of the permutation (otherwise it would be unfair). You can try to run it to see if it makes a difference.\nThe first code is to create the permutation order to keep constant across ROIs.\nThe second code is the one that actually runs the nested cross-validation for hyperparameter optimisation.\n\n\nfrom pathlib import Path\nimport numpy as np\n\ndef create_permutation_array_within(\n    project_root: str,\n    design: str,\n    permutations: int = 1000\n):\n\n    n_runs = 8\n\n    n_conditions = 8 if design == \"expanded\" else 2\n    values = np.array(range(n_runs * n_conditions))\n    groups = [[x] * n_conditions for x in range(n_runs)]\n    groups = [item for items in groups for item in items]\n    permutation_array = np.zeros([permutations,len(groups)])\n\n    for perm in range(permutations):\n        for index in np.unique(groups):\n            mask = groups==index\n            values[mask] = np.random.permutation(values[mask])\n\n        permutation_array[perm,:] = values.T\n\n    permutation_array = permutation_array\n    \n    np.savetxt(f\"{project_root}/data/utils/{design}_permutation_within.txt\",permutation_array, fmt='%i')\n\n\nproject_root=f\"{Path.home()}/implied_motion\"\n\ncreate_permutation_array_within(\n    project_root=project_root,\n    design='compact',\n    permutations=1000\n)\n\ncreate_permutation_array_within(\n    project_root=project_root,\n    design='expanded',\n    permutations=1000\n)\n\n\n\n\n#!/gpfs01/bartels/user/cdemircan/miniconda3/envs/lr_bartels/bin/python\n\n\n# this is an implementation of the real x real or implied x implied\n# decoding that I ended up not using, simply because the hyperparameter\n# optimisation in a nested cross validation for 1000 permutations\n# takes too much time (~4 days on CIN cluster for all combinations)\n# and I do not have enough time after to tweak the pipeline if things\n# need to change. Also the gains of this approach are unclear\n# and is an empirical question we can only see by running the code.\n# maybe you can try it :)\n\nimport argparse\nimport glob\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom joblib import Parallel, delayed\n\n\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import LeaveOneGroupOut, GridSearchCV\n\nfrom nilearn.glm.first_level import FirstLevelModel\nfrom nilearn.maskers import NiftiMasker\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--workingdir\", \"-w\")\nparser.add_argument(\"--sub\", \"-s\")\nparser.add_argument(\"--roi\", \"-r\")\nparser.add_argument(\"--targetdecode\", \"-t\")\nparser.add_argument(\"--design\", \"-d\")\nparser.add_argument(\"--permutations\", \"-p\")\nparser.add_argument(\"--condition\", \"-c\")\n\nargs = parser.parse_args()\nproject_root = args.workingdir\nparticipant = f\"sub-{str(args.sub).zfill(2)}\"\nroi = args.roi\ntarget_decode = args.targetdecode\ndesign = args.design\npermutations = int(args.permutations)\n\ncondition_dict = {'real':'train','implied':'test'}\ncondition = condition_dict[args.condition]\n\ntry:\n    def nested_cross_validate(\n        X,\n        y,\n        groups,\n        decode_pipeline,\n        parameter_grid\n    ):\n        inner_CV = LeaveOneGroupOut()\n        outer_CV = LeaveOneGroupOut()\n        scores = []\n        y = np.array(y)\n\n        for i,(train_index, test_index) in enumerate(outer_CV.split(X, y, groups=groups)):\n            X_tr, X_tt = X[train_index,:], X[test_index,:]\n            y_tr, y_tt = y[train_index], y[test_index]\n\n            inner_groups= [x for x in groups if x!=i]\n\n            inner_search = GridSearchCV(\n                clone(decode_pipeline),\n                param_grid=parameter_grid,\n                scoring=\"accuracy\",\n                cv=inner_CV,\n                n_jobs=-1)\n            inner_search.fit(X_tr,y_tr,groups=inner_groups)\n\n            pred = inner_search.score(X_tt,y_tt)   \n            scores.append(pred)\n        \n        return np.array(scores).mean()\n\n    functional_path = f\"{project_root}/data/derivatives/fMRIprep/{participant}/func\"\n\n    target_event_dict = {\"lr\": [\"left\", \"right\"], \"fb\": [\"forward\", \"backward\"]}\n    target_events = target_event_dict[target_decode]\n\n    TR = 1.2\n\n    # get file names\n    func_path = f\"{project_root}/data/derivatives/fMRIprep/{participant}/func\"\n\n    if design == \"compact\":\n        event_files = glob.glob(\n            f\"{project_root}/data/bids/{participant}/func/{participant}*events.tsv\"\n        )\n    elif design == \"expanded\":\n        event_files = glob.glob(\n            f\"{project_root}/data/bids/{participant}/func/expanded*events.tsv\"\n        )\n\n    target_files = glob.glob(\n        f\"{project_root}/data/bids/{participant}/func/{participant}*events.tsv\"\n    )\n    conf_files = glob.glob(f\"{func_path}/*confounds_timeseries.tsv\")\n    func_files = glob.glob(f\"{func_path}/*T1w_desc-preproc_bold.nii.gz\")\n\n    # filter by real motion\n    target_files = [x for x in target_files if condition in x]\n    event_files = [x for x in event_files if condition in x]\n    conf_files = [x for x in conf_files if condition in x]\n    func_files = [x for x in func_files if condition in x]\n\n    # sort them in the right order\n    target_files = sorted(\n        target_files, key=lambda x: int(x.split(\"run-\")[1].split(\"_events\")[0])\n    )\n    event_files = sorted(\n        event_files, key=lambda x: int(x.split(\"run-\")[1].split(\"_events\")[0])\n    )\n    conf_files = sorted(\n        conf_files, key=lambda x: int(x.split(\"run-\")[1].split(\"_desc\")[0])\n    )\n    func_files = sorted(\n        func_files, key=lambda x: int(x.split(\"run-\")[1].split(\"_space\")[0])\n    )\n\n    # read the files\n    targets = [pd.read_table(x) for x in target_files]\n    targets = [\n        x[(x[\"trial_type\"] == target_events[0]) | (x[\"trial_type\"] == target_events[1])]\n        for x in targets\n    ]\n\n    events = [pd.read_table(x) for x in event_files]\n    confs = [\n        pd.read_table(x)[[\"trans_x\", \"trans_y\", \"trans_z\", \"rot_x\", \"rot_y\", \"rot_z\"]]\n        for x in conf_files\n    ]\n\n    # get the ROI mask\n\n    roi_mask = f\"{project_root}/data/derivatives/fMRIprep/{participant}/ROI/{roi}.nii\"\n\n    effects = []\n    conditions_label = []\n    session_label = []\n\n    for session in range(len(func_files)):\n\n        glm = FirstLevelModel(\n            t_r=TR,\n            mask_img=roi_mask,\n            high_pass=0.01,\n            hrf_model=\"spm\",\n            smoothing_fwhm=None,\n            n_jobs=-1,\n            memory=None,\n        )\n\n        # fit the glm\n        glm.fit(func_files[session], events=events[session], confounds=confs[session])\n\n        # set up contrasts: one per condition\n        conditions = events[session][\n            events[session][\"trial_type\"].str.startswith(target_events[0])\n            | (events[session][\"trial_type\"].str.startswith(target_events[1]))\n        ][\"trial_type\"].unique()\n\n        # for the expanded glm targets need to repeat\n        if design == \"compact\":\n            current_targets = conditions\n        elif design == \"expanded\":\n            current_targets = []\n            for cond in conditions:\n                current_targets.append(cond.split(\"_\")[0])\n\n        # get z scored betas and labels with associated functional runs\n        for target, condition in zip(current_targets, conditions):\n\n            effects.append(glm.compute_contrast(condition, output_type=\"z_score\"))\n            conditions_label.append(target)\n            session_label.append(func_files[session])\n\n    n_runs = 8\n    n_conditions = 8 if design == \"expanded\" else 2\n    nifti_masker = NiftiMasker(\n        mask_img=roi_mask,\n        standardize=False,\n        runs=session_label,\n        smoothing_fwhm=None,\n        memory_level=1,\n    )\n\n    X = nifti_masker.fit_transform(effects)\n    y = np.array(conditions_label)\n    groups = [[x] * n_conditions for x in range(n_runs)]\n    groups = [item for items in groups for item in items]\n\n    classifier = LogisticRegression(\n        penalty=\"l2\", C=1.0, max_iter=4000, solver=\"lbfgs\", multi_class=\"auto\"\n    )\n\n    decode_pipeline = Pipeline([(\"logistic_regression\", classifier)])\n    parameter_grid = {\"logistic_regression__C\": np.arange(0.01, 10, 0.1)}\n\n    real_score = nested_cross_validate(\n        X,\n        y,\n        groups,\n        decode_pipeline,\n        parameter_grid\n    )\n\n    permutation_array = np.loadtxt(f\"{project_root}/data/utils/{design}_permutation_within.txt\").astype(int)\n\n    permutation_scores = Parallel(n_jobs=-1)\\\n    (delayed(nested_cross_validate)\\\n        (X,\n        y[permutation_array[i,:]],\n        groups,decode_pipeline,\n        parameter_grid\n        ) for i in range(permutations))\n\n    permutation_scores = list(permutation_scores)\n    permutation_scores.append(real_score)\n    decode_dict = {\n        \"accuracy\": permutation_scores,\n        \"type\": [\"permutation\"] * permutations + [\"real\"],\n        \"participant\": [participant] * (permutations + 1),\n        \"roi\": [roi] * (permutations + 1),\n        \"design\": [design] * (permutations + 1),\n        \"target\": [target_decode] * (permutations + 1),\n    }\n\n    df = pd.DataFrame(decode_dict)\n\n    df.to_csv(\n        f\"{project_root}/data/decoding/{args.condition}/{participant}_{target_decode}_{roi}_{design}.csv\",\n        index=False,\n    )\n\nexcept Exception as Argument:\n\n    f = open(f\"{project_root}/error_logs.txt\", \"a\")\n    f.write(str(Argument))\n    f.close()"
  },
  {
    "objectID": "behavioural_data_notes.html#training",
    "href": "behavioural_data_notes.html#training",
    "title": "Behavioural Data Notes",
    "section": "Training",
    "text": "Training\nThis experiment uses a block-based design and shows participants stimuli of real motion.\nIn each block, stimuli moving in one direction (left, right, forward, backward) is presented. Within a block, the speed of movement varies across trials, and participants are asked to press a button when the speed increases from one trial to the next.\nNote that running the experiment script (trainIMD or variants) creates only 1 run! So, for each participant, the script was run 8 times, and there were 8 log files per participant.\nThe presented conditions, number of blocks, velocity parameters, run duration etc. match with what is written on the script."
  },
  {
    "objectID": "behavioural_data_notes.html#test",
    "href": "behavioural_data_notes.html#test",
    "title": "Behavioural Data Notes",
    "section": "Test",
    "text": "Test\nThis experiment uses a block-based design and shows participants stimuli of implied motion.\nIn each block, stimuli moving in one direction (left, right, forward, backward) is presented. They performed a 1-back task to maintain attention on the images and pressed a key when the same image (matching both fore- and background) was presented the second time in a row, which occurred randomly once per block.\nNote that running the experiment script (testIMD or variants) creates only 1 run! So, for each participant, the script was run 8 times, and there were 8 log files per participant.\nThe presented conditions, number of blocks, velocity parameters, run duration etc. match with what is written on the script."
  },
  {
    "objectID": "behavioural_data_notes.html#stimulus-presentation-and-scanning-parameters",
    "href": "behavioural_data_notes.html#stimulus-presentation-and-scanning-parameters",
    "title": "Behavioural Data Notes",
    "section": "Stimulus Presentation and Scanning Parameters",
    "text": "Stimulus Presentation and Scanning Parameters\nEverything in this table is obtained from the matlab scripts only (except the correct TR). Some of these are to be cross-checked with the imaging data.\n\n\n\n\n\n\n\nparameter\nvalue\n\n\n\n\nwidth\n1280px\n\n\nheight\n1024 px\n\n\nresponse button\n4$ (pressed using right index finger)\n\n\ndummy TRs\n8 (plus there is an additional manual pulse from the researcher at the beginning)\n\n\nTR\n1.2 seconds (for the first three participants it is .6 seconds), but these data are discarded\n\n\nrefresh rate\n60 Hz\n\n\nbackground\ngrey [123,123,123]\n\n\ntrigger button\nw\n\n\nzoom in condition\n1\n\n\nzoom out condition\n2\n\n\nright movement condition\n3\n\n\nleft movement condition\n4\n\n\nspeeds of movement\n[4, 8, 12, 16, 20, 24 , 28, 32]\n\n\nimage per block\n8\n\n\nimage presentation duration\n1.5 seconds\n\n\ninter block interval\n8 seconds\n\n\ntotal block no per run\n16\n\n\ntotal number of runs\n8\n\n\ntotal functional images per run\nGizem reports 509 (including 8 dummies). However, this is based on the TR she manually enters, which is incorrect for the analysed data. Number of functional images is therefore half the reported number, which is 254\n\n\ntime of run (roughly)\n305 seconds"
  },
  {
    "objectID": "behavioural_data_notes.html#recorded-data-file-structure",
    "href": "behavioural_data_notes.html#recorded-data-file-structure",
    "title": "Behavioural Data Notes",
    "section": "Recorded Data File Structure",
    "text": "Recorded Data File Structure\nFiles are recorded as .mat files with the following name structure:\ne01s{participant no}_{block no}_{block type}_{date}.mat\nwhere block numbers go from 1 to 16 (mixed across block types) and block types are Train (real motion) or Test (implied motion).\nOn the first level of the struct, field contain a lot of meta-data that is either written above or found in the scripts. An important field is logs which contains different info for the train and test blocks. Here are the shared ones:\n\nblockSeq: Numbers from 1 to 4, each of which correspond to a condition (see above).\nblockOnsets: Onset time of the block\ntimeStart: start of the run (after dummies)\nfmri: start of the run (with dummies)\n\nFor Train, there are the additional following fields:\n\nspeedtask: 136 by 3 array. Each row is a trial. First column specifies the block (1 to 17). Second column specifies the velocity of stimulus. Third column specifies the onset of the trial.\ntKeyPresses: Timing of key presses\nblockIntTE: 17 by 2 array. Each row is a block. First column specifies the block type (1 to 4). Second column specifies the duration between blocks.\nblockOffsets: Offset time of the block\n\nFor Test, there are the additional following fields:\n\nifi: flip interval of the monitor\nblockImgSeq: 136 by 2 array. First column specifies the block type (1 to 4). Second column specifies the image identity no in a given trial.\ntBackMatches: An array of varying length (min 17) which specifies the timing of repeating stimuli. Note that this does not match the description of what is written in the manuscript, which is that images repeat only once per block.\ntaskKeyPresses: Response of participants when they thought they saw matching images\nblockOffsets: Offset time of the block PLUS the inter block interval\nbintOnset: Offset time of the block (excluding the interblock interval)"
  }
]